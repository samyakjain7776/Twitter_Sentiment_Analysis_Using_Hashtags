{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XnIpJnkHuT-D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kushagra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt \n",
    "import keras\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 759,
     "status": "ok",
     "timestamp": 1609279663355,
     "user": {
      "displayName": "Rohith Teja",
      "photoUrl": "",
      "userId": "17526535861407839040"
     },
     "user_tz": -60
    },
    "id": "VijxsP322W3W",
    "outputId": "2282a6cc-5ea8-430a-9b76-7c5075007a42"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>['id', 'have', 'responded', 'if', 'i', 'were',...</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>['soon', 'sad', 'i', 'will', 'miss', 'you', 'h...</td>\n",
       "      <td>sooo sad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>['my', 'boss', 'is', 'bullying', 'me']</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>['what', 'interview', 'leave', 'me', 'alone']</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>['sons', 'of', 'why', 'couldnt', 'they', 'put'...</td>\n",
       "      <td>sons of</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>['wish', 'we', 'could', 'come', 'see', 'u', 'o...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>['ive', 'wondered', 'about', 'rake', 'to', 'th...</td>\n",
       "      <td>dont force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>['yay', 'good', 'for', 'both', 'of', 'you', 'e...</td>\n",
       "      <td>yay good for both of you</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>['but', 'it', 'was', 'worth', 'it']</td>\n",
       "      <td>but it was worth it</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>['all', 'this', 'flirting', 'going', 'on', 'th...</td>\n",
       "      <td>all this flirting going on the atg smiles yay ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1  ['id', 'have', 'responded', 'if', 'i', 'were',...   \n",
       "1      549e992a42  ['soon', 'sad', 'i', 'will', 'miss', 'you', 'h...   \n",
       "2      088c60f138             ['my', 'boss', 'is', 'bullying', 'me']   \n",
       "3      9642c003ef      ['what', 'interview', 'leave', 'me', 'alone']   \n",
       "4      358bd9e861  ['sons', 'of', 'why', 'couldnt', 'they', 'put'...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0  ['wish', 'we', 'could', 'come', 'see', 'u', 'o...   \n",
       "27477  4f4c4fc327  ['ive', 'wondered', 'about', 'rake', 'to', 'th...   \n",
       "27478  f67aae2310  ['yay', 'good', 'for', 'both', 'of', 'you', 'e...   \n",
       "27479  ed167662a5                ['but', 'it', 'was', 'worth', 'it']   \n",
       "27480  6f7127d9d7  ['all', 'this', 'flirting', 'going', 'on', 'th...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                      id have responded if i were going   neutral  \n",
       "1                                               sooo sad  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                               sons of   negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                         dont force  negative  \n",
       "27478                           yay good for both of you  positive  \n",
       "27479                               but it was worth it   positive  \n",
       "27480  all this flirting going on the atg smiles yay ...   neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/preprocessed_train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sEBirsml1Hgu"
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "df = pd.read_csv(\"data/preprocessed_train.csv\")\n",
    "df.text = df.selected_text.map(lambda x:str(x))\n",
    "df.sentiment = df.sentiment.astype(\"category\")\n",
    "df.sentiment = df.sentiment.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Training, Testing and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Kiv4fMpy2wVQ"
   },
   "outputs": [],
   "source": [
    "# train, val, test split\n",
    "x_train, xtest, y_train, ytest = train_test_split(df.text.values, df.sentiment.values,stratify=df.sentiment.values, test_size=0.3,random_state=1)\n",
    "y_train = to_categorical(y_train)\n",
    "x_val = xtest[0:4122]\n",
    "y_val = to_categorical(ytest[0:4122])\n",
    "x_test = xtest[4122:]\n",
    "y_test = ytest[4122:]\n",
    "\n",
    "\n",
    "#tokenizing and padding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df.text.values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(x_train)\n",
    "X_val = tokenizer.texts_to_sequences(x_val)\n",
    "X_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MfchPQ_iuUHb"
   },
   "outputs": [],
   "source": [
    "#import glove embeddings\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('glove.twitter.27B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Train the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z4oLKEuduUTR"
   },
   "outputs": [],
   "source": [
    "#using glove\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def cnn_glove(activation,optimizer,epochs,batchsize):\n",
    "  embedding_dim = 100\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                          output_dim=embedding_dim,\n",
    "                          weights=[embedding_matrix],\n",
    "                          input_length=maxlen))\n",
    "  model.add(Conv1D(32, kernel_size=2, activation=activation))\n",
    "  model.add(layers.MaxPooling1D(2))\n",
    "  model.add(Conv1D(32, kernel_size=2, activation=activation))\n",
    "  model.add(Dropout(0.25))\n",
    "  model.add(Conv1D(16, kernel_size=2, activation=activation))\n",
    "  model.add(layers.MaxPooling1D(2))\n",
    "  model.add(Conv1D(16, kernel_size=2, activation=activation))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Flatten())\n",
    "\n",
    "  model.add(Dense(64, activation='tanh'))\n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "  model.add(Dense(3, activation='softmax'))\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  history = model.fit(X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=batchsize)\n",
    "  \n",
    "  return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DD3YOLNLfVq5"
   },
   "outputs": [],
   "source": [
    "activation = [\"selu\", \"elu\", \"sigmoid\", \"tanh\"]\n",
    "optimizer = [\"adam\", \"SGD\", \"RMSprop\", \"Adadelta\"]\n",
    "epochs = [5,10,15,20]\n",
    "batchsize = [8,16,32,64,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. selecting activation fixing - optimizer = adam, epochs = 5, batch = 16\n",
    "sel_activation = {}\n",
    "for i in activation:\n",
    "  history, model = cnn_glove(i,\"adam\",5,16)\n",
    "  temp = {i:model.evaluate(X_val,y_val)[1]}\n",
    "  sel_activation.update(temp)\n",
    "  keras.backend.clear_session()\n",
    "\n",
    "sel_activation_final = max(sel_activation, key=sel_activation.get)\n",
    "print(\"best activation function is \",sel_activation_final)\n",
    "\n",
    "# 2. selecting optimizer by fixing - activation = best, epochs = 5, batch = 16\n",
    "sel_optimizer = {}\n",
    "for i in optimizer:\n",
    "  history, model = cnn_glove(sel_activation_final,i,5,16)\n",
    "  temp = {i:model.evaluate(X_val,y_val)[1]}\n",
    "  sel_optimizer.update(temp)\n",
    "  keras.backend.clear_session()\n",
    "\n",
    "sel_optimizer_final = max(sel_optimizer, key=sel_optimizer.get)\n",
    "print(\"best optimizer is \",sel_optimizer_final)\n",
    "\n",
    "# 3.  graph epoch vs accuracy score\n",
    "\n",
    "acc_train_epoch = {}\n",
    "acc_val_epoch = {}\n",
    "for i in epochs:\n",
    "  history, model = cnn_glove(sel_activation_final,sel_optimizer_final,i,16)\n",
    "  temp_train = {i:model.evaluate(X_train,y_train)[1]}\n",
    "  temp_val = {i:model.evaluate(X_val,y_val)[1]}\n",
    "  acc_train_epoch.update(temp_train)\n",
    "  acc_val_epoch.update(temp_val)\n",
    "  keras.backend.clear_session()\n",
    "\n",
    "sel_epoch_final = max(acc_val_epoch, key=acc_val_epoch.get)\n",
    "print(\"best epoch is \",sel_epoch_final)\n",
    "\n",
    "df_epoch_train = pd.DataFrame(list(acc_train_epoch.items()), columns=['Epochs', 'Accuracy score'])\n",
    "df_epoch_val = pd.DataFrame(list(acc_val_epoch.items()), columns=['Epochs', 'Accuracy score'])\n",
    "\n",
    "df_epoch_val.Epochs = df_epoch_val.Epochs.map(lambda x:str(x))\n",
    "df_epoch_train.Epochs = df_epoch_train.Epochs.map(lambda x:str(x))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_epoch_train.iloc[:,0],df_epoch_train.iloc[:,1],c=\"r\",label=\"train\",linestyle='--', marker='o')\n",
    "plt.plot(df_epoch_val.iloc[:,0],df_epoch_val.iloc[:,1],c=\"b\",label = \"val\",linestyle='--', marker='o')\n",
    "plt.title(\"Accuracy score vs Epochs\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"images/acc-epoch-glove-\"+case, bbox_inches='tight',dpi = 200)\n",
    "\n",
    "\n",
    "# 4. graph batch size vs accuracy score\n",
    "acc_train_batch = {}\n",
    "acc_val_batch = {}\n",
    "for i in batchsize:\n",
    "  history, model = cnn_glove(sel_activation_final,sel_optimizer_final,sel_epoch_final,i)\n",
    "  temp_train = {i:model.evaluate(X_train,y_train)[1]}\n",
    "  temp_val = {i:model.evaluate(X_val,y_val)[1]}\n",
    "  acc_train_batch.update(temp_train)\n",
    "  acc_val_batch.update(temp_val)\n",
    "  keras.backend.clear_session()\n",
    "\n",
    "sel_batch_final = max(acc_val_batch, key=acc_val_batch.get)\n",
    "print(\"best batchsize is \",sel_batch_final)\n",
    "\n",
    "df_batch_train = pd.DataFrame(list(acc_train_batch.items()), columns=['Batchsize', 'Accuracy score'])\n",
    "df_batch_val = pd.DataFrame(list(acc_val_batch.items()), columns=['Batchsize', 'Accuracy score'])\n",
    "\n",
    "df_batch_val.Batchsize = df_batch_val.Batchsize.map(lambda x:str(x))\n",
    "df_batch_train.Batchsize = df_batch_train.Batchsize.map(lambda x:str(x))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_batch_train.iloc[:,0],df_batch_train.iloc[:,1],c=\"r\",label=\"train\",linestyle='--', marker='o')\n",
    "plt.plot(df_batch_val.iloc[:,0],df_batch_val.iloc[:,1],c=\"b\",label = \"val\",linestyle='--', marker='o')\n",
    "plt.title(\"Accuracy score vs Batchsize\")\n",
    "plt.ylabel(\"Accuracy score\")\n",
    "plt.xlabel(\"Batchsize\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"images/acc-batch-glove-\"+case, bbox_inches='tight',dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1570943,
     "status": "ok",
     "timestamp": 1609197778115,
     "user": {
      "displayName": "Rohith Teja",
      "photoUrl": "https://lh3.googleusercontent.com/-nt8x4joQmgY/AAAAAAAAAAI/AAAAAAAAAvg/AbgIIUozOq0/s64/photo.jpg",
      "userId": "01155222072916958278"
     },
     "user_tz": -60
    },
    "id": "dk16QKlGk3y7",
    "outputId": "72923228-ab75-44bd-c350-45c322bd3ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best activation function is  selu\n",
      "best optimizer is  RMSprop\n",
      "best epoch is  5\n",
      "best batchsize is  8\n",
      "test accuracy score =  0.8430754305117633\n",
      "test f1 score =  0.8418506662992482\n",
      "time taken is  92.29925203323364\n"
     ]
    }
   ],
   "source": [
    "#experiments using glove embeddings\n",
    "sel_activation_final = 'selu'\n",
    "print(\"best activation function is \",sel_activation_final)\n",
    "\n",
    "sel_optimizer_final = 'RMSprop'\n",
    "print(\"best optimizer is \",sel_optimizer_final)\n",
    "\n",
    "sel_epoch_final = 5\n",
    "print(\"best epoch is \",sel_epoch_final)\n",
    "\n",
    "sel_batch_final = 8\n",
    "print(\"best batchsize is \",sel_batch_final)\n",
    "\n",
    "# Training the CNN Model\n",
    "t0 = time()\n",
    "\n",
    "history, model = cnn_glove(sel_activation_final,sel_optimizer_final,sel_epoch_final,sel_batch_final)\n",
    "pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "print(\"test accuracy score = \",accuracy_score(y_pred=pred, y_true=y_test))\n",
    "print(\"test f1 score = \",f1_score(y_pred=pred, y_true=y_test, average=\"weighted\"))\n",
    "\n",
    "t1 = time()\n",
    "print(\"time taken is \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          1785200   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 99, 32)            6432      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 49, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 48, 32)            2080      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 47, 16)            1040      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 22, 16)            528       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 352)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                22592     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,818,067\n",
      "Trainable params: 1,818,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cnn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id have responded if i were going\n"
     ]
    }
   ],
   "source": [
    "print(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Negative\n",
      "1: Neutral\n",
      "2: Positive\n",
      "\n",
      "[0 1 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "inp = ['Disappointing performance by CSK today',\n",
    "'I`d have responded, if I were going',\n",
    "'I hate the rain',\n",
    "'I love the music',\n",
    "'it was just the handle i guess'\n",
    "]\n",
    "X_test1 = tokenizer.texts_to_sequences(inp)\n",
    "X_test1 = pad_sequences(X_test1, padding='post', maxlen = maxlen)\n",
    "print('0: Negative\\n1: Neutral\\n2: Positive\\n')\n",
    "print(np.argmax(model.predict(X_test1), axis = -1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2. CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
